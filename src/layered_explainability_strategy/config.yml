DecisionTreeClassifier:
  criterion: "entropy"
  splitter: "random"
#  max_depth: [ null, 5, 14, 18, 20 ]
  min_samples_split: 3
  ccp_alpha: 0.001

RidgeClassifier:
  solver: auto  # lbfgs when positive = True
  tol: 0.0001
  alpha: 2.5
  positive: false

gs:
  DecisionTreeClassifier:
    criterion: [ "gini", "entropy", "log_loss" ]
    splitter: [ "best", "random" ]
    max_depth: [ null, 5, 14, 18, 20 ]
    min_samples_split: [ 2, 3, 4 ]
    ccp_alpha: [ 0.0, 0.001, 0.0001, 0.01, 0.1 ]

  RidgeClassifier:
    solver: [ auto ] # lbfgs when positive = True
    tol: [ 1.0e-4, 1.0e-5, 1.0e-3, 1.0e-2 ]
    alpha: [ 1.0, 2.5, 5.0, 7.5, 10.0, 15.0 ]
    positive: [ false, true ]

  LogisticRegression:
    solver: [ liblinear, newton-cg, lbfgs ]
    tol: [ 1.0e-4, 1.0e-5, 1.0e-3 ]
    dual: [ true, false ]
    penalty: [ l1, l2 ]
    C: [ 0.001, 0.01, 0.1, 1.0, 10, 100, 1000 ]
    max_iter: [ 500, 1000 ]